<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://nexentaedge.io/blog</id>
    <title>NexentaEdge Blog</title>
    <updated>2018-06-13T06:00:00Z</updated>
    <generator>Feed for Node.js</generator>
    <link rel="alternate" href="http://nexentaedge.io/blog"/>
    <subtitle>The best place to stay up-to-date with the latest NexentaEdge news and events.</subtitle>
    <logo>http://nexentaedge.io/img/logo-nexenta-full.png</logo>
    <rights>Copyright © 2018 Nexenta Systems</rights>
    <entry>
        <title type="html"><![CDATA[Snapshot Manifests]]></title>
        <id>http://nexentaedge.io/blog/2018/06/13/SnapshotManifests.html</id>
        <link href="http://nexentaedge.io/blog/2018/06/13/SnapshotManifests.html">
        </link>
        <updated>2018-06-13T06:00:00Z</updated>
        <summary type="html"><![CDATA[<p>Snapshot Manifests are NexentaEdge's implementation of a traditional copy-on-write snapshot applied to a fully distributed eventually consistent storage cluster.</p>
<p>Snapshot Manifests are versioned objects which specify a precise set of object versions</p>
]]></summary>
        <author>
            <name>Caitlin bestler</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unique Version IDs and Generations, but no Version Numbers]]></title>
        <id>http://nexentaedge.io/blog/2018/06/12/UniqueVersionsButNoVersionNumbers.html</id>
        <link href="http://nexentaedge.io/blog/2018/06/12/UniqueVersionsButNoVersionNumbers.html">
        </link>
        <updated>2018-06-12T06:00:00Z</updated>
        <summary type="html"><![CDATA[<p>How do you assign a unique version number for a new object version even when the network is split? Two clients, A and B, both want to create a new version of Object X, but there is no network connectivity between them. What do you do?</p>
<ul>
<li>Only allow th</li>
</ul>
]]></summary>
        <author>
            <name>Caitlin Bestler</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature that made ZFS famous]]></title>
        <id>http://nexentaedge.io/blog/2018/04/25/Feature-that-made-ZFS-famous.html</id>
        <link href="http://nexentaedge.io/blog/2018/04/25/Feature-that-made-ZFS-famous.html">
        </link>
        <updated>2018-04-25T06:00:00Z</updated>
        <summary type="html"><![CDATA[<p>It was back in 2005 when Sun Microsystems unveiled OpenSolaris and young yet brave new file system called ZFS. Sun did lots of marketing to get ZFS widely recognized and known. Mostly it was grass root effort via engineering blogs and forums, people</p>
]]></summary>
        <author>
            <name>Dmitry Yusupov</name>
            <uri>http://twitter.com/dmitryy</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Tenant Access To A Shared Storage Cluster]]></title>
        <id>http://nexentaedge.io/blog/2018/04/12/MultipleTenantAccessToSharedStorage.html</id>
        <link href="http://nexentaedge.io/blog/2018/04/12/MultipleTenantAccessToSharedStorage.html">
        </link>
        <updated>2018-04-12T06:00:00Z</updated>
        <summary type="html"><![CDATA[<p>In the prior blog we discussed how to use Kubernetes to provision a class of storage clusters which protects against loss of stored assets itself via erasure encoding and/or replication across already provisioned resources, rather than relying on Kub</p>
]]></summary>
        <author>
            <name>Caitlin Bestler</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond the Virtual Disk]]></title>
        <id>http://nexentaedge.io/blog/2018/04/12/BeyondVirtualDisks.html</id>
        <link href="http://nexentaedge.io/blog/2018/04/12/BeyondVirtualDisks.html">
        </link>
        <updated>2018-04-12T06:00:00Z</updated>
        <summary type="html"><![CDATA[<p>Kubernetes defines numerous options for Persistent Volumes and Persistent Volume Claims. The problem is that they are too numerous, or perhaps more to the point too diverse.</p>
<p>A Persistent Volume can be anything from a set of storage held by a backend</p>
]]></summary>
        <author>
            <name>Caitlin Bestler</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Immutable Metadata Not Enough]]></title>
        <id>http://nexentaedge.io/blog/2018/03/30/ImmutableMetadataNotEnough.html</id>
        <link href="http://nexentaedge.io/blog/2018/03/30/ImmutableMetadataNotEnough.html">
        </link>
        <updated>2018-03-30T06:00:00Z</updated>
        <summary type="html"><![CDATA[<p>In prior blogs I've explained how NexentaEdge has immutable self-validating location-independent metadata referencing self-validating location-independent payload. The same can be set about IPFS, the Interplanetary File System (<a href="https://ipfs.io">https://ipfs.io</a>). Whil</p>
]]></summary>
        <author>
            <name>Caitlin Bestler</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Namespace Manifests]]></title>
        <id>http://nexentaedge.io/blog/2018/03/26/NamespaceManifests.html</id>
        <link href="http://nexentaedge.io/blog/2018/03/26/NamespaceManifests.html">
        </link>
        <updated>2018-03-26T06:00:00Z</updated>
        <summary type="html"><![CDATA[<p>With efficient group messaging a group of storage targets can efficiently manage the collective responsibility for storing Chunks within the group while allowing metadata references to the stored chunks to omit the specific storage targets selected.</p>
]]></summary>
        <author>
            <name>Caitlin Bestler</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Location Independent References]]></title>
        <id>http://nexentaedge.io/blog/2018/03/22/LocationIndependentReferences.html</id>
        <link href="http://nexentaedge.io/blog/2018/03/22/LocationIndependentReferences.html">
        </link>
        <updated>2018-03-22T06:00:00Z</updated>
        <summary type="html"><![CDATA[<p>In the prior blog on NexentaEdge we mentioned that
Chunks were unique and immutable and that Chunk References
merely identify how a Chunk is used to rebuild an object,
but do not specify the locations where the chunk is stored.</p>
<p>This time we will exp</p>
]]></summary>
        <author>
            <name>Caitlin Bestler</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consensus, Who Needs It?]]></title>
        <id>http://nexentaedge.io/blog/2018/03/20/ConsensusNotNeeded.html</id>
        <link href="http://nexentaedge.io/blog/2018/03/20/ConsensusNotNeeded.html">
        </link>
        <updated>2018-03-20T06:00:00Z</updated>
        <summary type="html"><![CDATA[<p>The conventional tradeoff for distributed storage clusters is between transactional consistency and eventual consistency. Eventual consistency is usually viewed as the cheaper solution, both in terms of desirability and system cost. The critical cost</p>
]]></summary>
        <author>
            <name>Caitlin Bestler</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Supporting Directory Operations With a Flat Name Index]]></title>
        <id>http://nexentaedge.io/blog/2016/06/14/DiretoryOperationsOnFlatNamespace.html</id>
        <link href="http://nexentaedge.io/blog/2016/06/14/DiretoryOperationsOnFlatNamespace.html">
        </link>
        <updated>2016-06-14T06:00:00Z</updated>
        <summary type="html"><![CDATA[<p>Traditional file systems allow renaming of directories or moving files from one directory to another, but with a giant asterisk. Rename operations are not allowed unless the existing and new names are both part of the same file system.</p>
<p>You probably</p>
]]></summary>
        <author>
            <name>Caitlin Bestler</name>
        </author>
    </entry>
</feed>