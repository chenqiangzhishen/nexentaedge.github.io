<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://nexentaedge.io/blog</id>
    <title>NexentaEdge Blog</title>
    <updated>2018-04-25T06:00:00Z</updated>
    <generator>Feed for Node.js</generator>
    <link rel="alternate" href="http://nexentaedge.io/blog"/>
    <subtitle>The best place to stay up-to-date with the latest NexentaEdge news and events.</subtitle>
    <logo>http://nexentaedge.io/img/logo-nexenta-full.png</logo>
    <rights>Copyright Â© 2018 Nexenta Systems</rights>
    <entry>
        <title type="html"><![CDATA[Feature that made ZFS famous]]></title>
        <id>http://nexentaedge.io/blog/2018/04/25/Feature-that-made-ZFS-famous.html</id>
        <link href="http://nexentaedge.io/blog/2018/04/25/Feature-that-made-ZFS-famous.html">
        </link>
        <updated>2018-04-25T06:00:00Z</updated>
        <summary type="html"><![CDATA[<p>It was back in 2005 when Sun Microsystems unveiled OpenSolaris and young yet brave new file system called ZFS. Sun did lots of marketing to get ZFS widely recognized and known. Mostly it was grass root effort via engineering blogs and forums, people</p>
]]></summary>
        <author>
            <name>Dmitry Yusupov</name>
            <uri>http://twitter.com/dmitryy</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Tenant Access To A Shared Storage Cluster]]></title>
        <id>http://nexentaedge.io/blog/2018/04/12/MultipleTenantAccessToSharedStorage.html</id>
        <link href="http://nexentaedge.io/blog/2018/04/12/MultipleTenantAccessToSharedStorage.html">
        </link>
        <updated>2018-04-12T06:00:00Z</updated>
        <summary type="html"><![CDATA[<p>In the prior blog we discussed how to use Kubernetes to provision a class of storage clusters which protects against loss of stored assets itself via erasure encoding and/or replication across already provisioned resources, rather than relying on Kub</p>
]]></summary>
        <author>
            <name>Caitlin Bestler</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond the Virtual Disk]]></title>
        <id>http://nexentaedge.io/blog/2018/04/12/BeyondVirtualDisks.html</id>
        <link href="http://nexentaedge.io/blog/2018/04/12/BeyondVirtualDisks.html">
        </link>
        <updated>2018-04-12T06:00:00Z</updated>
        <summary type="html"><![CDATA[<p>Kubernetes defines numerous options for Persistent Volumes and Persistent Volume Claims. The problem is that they are too numerous, or perhaps more to the point too diverse.</p>
<p>A Persistent Volume can be anything from a set of storage held by a backend</p>
]]></summary>
        <author>
            <name>Caitlin Bestler</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Immutable Metadata Not Enough]]></title>
        <id>http://nexentaedge.io/blog/2018/03/30/ImmutableMetadataNotEnough.html</id>
        <link href="http://nexentaedge.io/blog/2018/03/30/ImmutableMetadataNotEnough.html">
        </link>
        <updated>2018-03-30T06:00:00Z</updated>
        <summary type="html"><![CDATA[<p>In prior blogs I've explained how NexentaEdge has immutable self-validating location-independent metadata referencing self-validating location-independent payload. The same can be set about IPFS, the Interplanetary File System (<a href="https://ipfs.io">https://ipfs.io</a>). Whil</p>
]]></summary>
        <author>
            <name>Caitlin Bestler</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Namespace Manifests]]></title>
        <id>http://nexentaedge.io/blog/2018/03/26/NamespaceManifests.html</id>
        <link href="http://nexentaedge.io/blog/2018/03/26/NamespaceManifests.html">
        </link>
        <updated>2018-03-26T06:00:00Z</updated>
        <summary type="html"><![CDATA[<p>With efficient group messaging a group of storage targets can efficiently manage the collective responsibility for storing Chunks within the group while allowing metadata references to the stored chunks to omit the specific storage targets selected.</p>
]]></summary>
        <author>
            <name>Caitlin Bestler</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Location Independent References]]></title>
        <id>http://nexentaedge.io/blog/2018/03/22/LocationIndependentReferences.html</id>
        <link href="http://nexentaedge.io/blog/2018/03/22/LocationIndependentReferences.html">
        </link>
        <updated>2018-03-22T06:00:00Z</updated>
        <summary type="html"><![CDATA[<p>In the prior blog on NexentaEdge we mentioned that
Chunks were unique and immutable and that Chunk References
merely identify how a Chunk is used to rebuild an object,
but do not specify the locations where the chunk is stored.</p>
<p>This time we will exp</p>
]]></summary>
        <author>
            <name>Caitlin Bestler</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consensus, Who Needs It?]]></title>
        <id>http://nexentaedge.io/blog/2018/03/20/ConsensusNotNeeded.html</id>
        <link href="http://nexentaedge.io/blog/2018/03/20/ConsensusNotNeeded.html">
        </link>
        <updated>2018-03-20T06:00:00Z</updated>
        <summary type="html"><![CDATA[<p>The conventional tradeoff for distributed storage clusters is between transactional consistency and eventual consistency. Eventual consistency is usually viewed as the cheaper solution, both in terms of desirability and system cost. The critical cost</p>
]]></summary>
        <author>
            <name>Caitlin Bestler</name>
        </author>
    </entry>
</feed>