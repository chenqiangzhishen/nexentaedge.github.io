<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>NexentaEdge Blog</title>
        <link>http://nexentaedge.io/blog</link>
        <description>The best place to stay up-to-date with the latest NexentaEdge news and events.</description>
        <lastBuildDate>Thu, 12 Apr 2018 06:00:00 GMT</lastBuildDate>
        <docs>http://blogs.law.harvard.edu/tech/rss</docs>
        <generator>Feed for Node.js</generator>
        <image>
            <title>NexentaEdge Blog</title>
            <url>http://nexentaedge.io/img/logo-nexenta-full.png</url>
            <link>http://nexentaedge.io/blog</link>
        </image>
        <copyright>Copyright Â© 2018 Nexenta Systems</copyright>
        <item>
            <title><![CDATA[Multiple Tenant Access To A Shared Storage Cluster]]></title>
            <link>http://nexentaedge.io/blog/2018/04/12/MultipleTenantAccessToSharedStorage.html</link>
            <guid>http://nexentaedge.io/blog/2018/04/12/MultipleTenantAccessToSharedStorage.html</guid>
            <pubDate>Thu, 12 Apr 2018 06:00:00 GMT</pubDate>
            <description><![CDATA[<p>In the prior blog we discussed how to use Kubernetes to provision a class of storage clusters which protects against loss of stored assets itself via erasure encoding and/or replication across already provisioned resources, rather than relying on Kub</p>
]]></description>
        </item>
        <item>
            <title><![CDATA[Beyond the Virtual Disk]]></title>
            <link>http://nexentaedge.io/blog/2018/04/12/BeyondVirtualDisks.html</link>
            <guid>http://nexentaedge.io/blog/2018/04/12/BeyondVirtualDisks.html</guid>
            <pubDate>Thu, 12 Apr 2018 06:00:00 GMT</pubDate>
            <description><![CDATA[<p>Kubernetes defines numerous options for Persistent Volumes and Persistent Volume Claims. The problem is that they are too numerous, or perhaps more to the point too diverse.</p>
<p>A Persistent Volume can be anything from a set of storage held by a backend</p>
]]></description>
        </item>
        <item>
            <title><![CDATA[Immutable Metadata Not Enough]]></title>
            <link>http://nexentaedge.io/blog/2018/03/30/ImmutableMetadataNotEnough.html</link>
            <guid>http://nexentaedge.io/blog/2018/03/30/ImmutableMetadataNotEnough.html</guid>
            <pubDate>Fri, 30 Mar 2018 06:00:00 GMT</pubDate>
            <description><![CDATA[<p>In prior blogs I've explained how NexentaEdge has immutable self-validating location-independent metadata referencing self-validating location-independent payload. The same can be set about IPFS, the Interplanetary File System (<a href="https://ipfs.io">https://ipfs.io</a>). Whil</p>
]]></description>
        </item>
        <item>
            <title><![CDATA[Namespace Manifests]]></title>
            <link>http://nexentaedge.io/blog/2018/03/26/NamespaceManifests.html</link>
            <guid>http://nexentaedge.io/blog/2018/03/26/NamespaceManifests.html</guid>
            <pubDate>Mon, 26 Mar 2018 06:00:00 GMT</pubDate>
            <description><![CDATA[<p>With efficient group messaging a group of storage targets can efficiently manage the collective responsibility for storing Chunks within the group while allowing metadata references to the stored chunks to omit the specific storage targets selected.</p>
]]></description>
        </item>
        <item>
            <title><![CDATA[Location Independent References]]></title>
            <link>http://nexentaedge.io/blog/2018/03/22/LocationIndependentReferences.html</link>
            <guid>http://nexentaedge.io/blog/2018/03/22/LocationIndependentReferences.html</guid>
            <pubDate>Thu, 22 Mar 2018 06:00:00 GMT</pubDate>
            <description><![CDATA[<p>In the prior blog on NexentaEdge we mentioned that
Chunks were unique and immutable and that Chunk References
merely identify how a Chunk is used to rebuild an object,
but do not specify the locations where the chunk is stored.</p>
<p>This time we will exp</p>
]]></description>
        </item>
        <item>
            <title><![CDATA[Consensus, Who Needs It?]]></title>
            <link>http://nexentaedge.io/blog/2018/03/20/ConsensusNotNeeded.html</link>
            <guid>http://nexentaedge.io/blog/2018/03/20/ConsensusNotNeeded.html</guid>
            <pubDate>Tue, 20 Mar 2018 06:00:00 GMT</pubDate>
            <description><![CDATA[<p>The conventional tradeoff for distributed storage clusters is between transactional consistency and eventual consistency. Eventual consistency is usually viewed as the cheaper solution, both in terms of desirability and system cost. The critical cost</p>
]]></description>
        </item>
        <item>
            <title><![CDATA[One SDS feature that made ZFS famous]]></title>
            <link>http://nexentaedge.io/blog/2016/04/25/One-SDS-feature-that-made-ZFS-famous.html</link>
            <guid>http://nexentaedge.io/blog/2016/04/25/One-SDS-feature-that-made-ZFS-famous.html</guid>
            <pubDate>Mon, 25 Apr 2016 06:00:00 GMT</pubDate>
            <description><![CDATA[<p>It was back in 2005 when Sun Microsystems unveiled OpenSolaris and an with it young yet brave new file system called ZFS. Clearly Sun did lots of marketing to get ZFS widely recognized and known. Mostly it was grass root effort via engineering blogs</p>
]]></description>
        </item>
    </channel>
</rss>