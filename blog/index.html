<html lang="en"><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Blog · NexentaEdge</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta property="og:title" content="Blog · NexentaEdge"/><meta property="og:type" content="website"/><meta property="og:url" content="https://nexentaedge.github.io/index.html"/><meta property="og:description" content="Your universal Scale-Out Storage Software with global Deduplication and Compression"/><link rel="shortcut icon" href="/img/favicon.png"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="alternate" type="application/atom+xml" href="https://nexentaedge.github.io/blog/atom.xml" title="NexentaEdge Blog ATOM Feed"/><link rel="alternate" type="application/rss+xml" href="https://nexentaedge.github.io/blog/feed.xml" title="NexentaEdge Blog RSS Feed"/><link rel="stylesheet" href="/css/fonts.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="https://use.fontawesome.com/releases/v5.0.8/js/all.js"></script><link rel="stylesheet" href="/css/main.css"/></head><body><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/logo-nexenta-edge.png"/><h2 class="headerTitle">NexentaEdge</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li><a href="https://groups.google.com/forum/#!forum/nexentaedge-users" target="_self">Ask us</a></li><li><a href="/docs/introduction.html" target="_self">Documentation</a></li><li><a href="/blog" target="_self">Blog</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search"/></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="container docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><i></i></div><h2><i>›</i><span>Recent Posts</span></h2></div><div class="navGroups"><div class="navGroup navGroupActive"><h3>Recent Posts</h3><ul><li class="navListItem"><a class="navItem" href="/blog/2018/03/30/ImmutableMetadataNotEnough.html">Immutable Metadata Not Enough</a></li><li class="navListItem"><a class="navItem" href="/blog/2018/03/26/NamespaceManifests.html">Namespace Manifests</a></li><li class="navListItem"><a class="navItem" href="/blog/2018/03/22/LocationIndependentReferences.html">Location Independent References</a></li><li class="navListItem"><a class="navItem" href="/blog/2018/03/20/ConsensusNotNeeded.html">Consensus, Who Needs It?</a></li><li class="navListItem"><a class="navItem" href="/blog/2016/03/11/blog-post.html">One SDS feature that made ZFS famous</a></li></ul></div></div></section></div><script>
          var toggler = document.getElementById('navToggler');
          var nav = document.getElementById('docsNav');
          toggler.onclick = function() {
            nav.classList.toggle('docsSliderActive');
          };
        </script></nav></div><div class="container mainContainer documentContainer postContainer blogContainer"><div class="wrapper"><div class="posts"><div class="post"><header class="postHeader"><h1><a href="/blog/2018/03/30/ImmutableMetadataNotEnough.html">Immutable Metadata Not Enough</a></h1><p class="post-meta">March 30, 2018</p><div class="authorBlock"><p class="post-authorName"><a target="_blank">Caitlin Bestler</a></p></div></header><article class="post-content"><div><span><p>In prior blogs I've explained how NexentaEdge has immutable self-validating location-independent metadata referencing self-validating location-independent payload. The same can be set about IPFS, the Interplanetary File System (<a href="https://ipfs.io">https://ipfs.io</a>). While the two storage solutions' handling of payload chunks is very similar, the differences in how objects are named and found are almost as different as possible.</p>
<h2><a class="anchor" aria-hidden="true" name="payload-chunks"></a><a href="#payload-chunks" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Payload Chunks</h2>
<p>The end result of putting a chunk to IPFS is that it is identified and validated with a cryptographic hash, and that the crypographic hash can be used to find the chunk for retrieval.</p>
<p>This is very similar to NexentaEdge, but there are differences:</p>
<ul>
<li>IPFS accepts the chunk and then generates its cryptographic hash. A NexentaEdge client directly interfacing to NexentaEdge cryptographically hashes the chunk before requesting that it be put. This avoids tranmission of duplicate chunks.</li>
<li>IPFS routing is a consistent hashing solution. NexentaEdge hashes to a Target Group and then does rapid negotiations within the group to find and dynamically place new chunks on the least burdened targets.</li>
</ul>
<h2><a class="anchor" aria-hidden="true" name="different-metadata-philosophy"></a><a href="#different-metadata-philosophy" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Different Metadata Philosophy</h2>
<p>The IPFS naming system is still a work-in-progress, but all of their examples suggest a very different method for publishing content accessible by name.</p>
<p>They take the cryptographic hash of the atomic object and embed those references in other documents, which basically function as directories. Each of these directory objects is also immutable, referencing specific frozen-in-time content. The directory object itself has a cryptographic hash, which can be referenced in higher layer directories. Finally a &quot;root&quot; directory is published which is then pointed to by a mutable name to directory object mappping.</p>
<p>From the examples given and the suggested implementations it is clear that this is not intended as a high transaction rate solution. This is something more akin to publishing the daily release of a open-source project. This new root is collected, authorized and published by a single authorative user.</p>
<p>This is not that bad of an approach for creating a &quot;permanent web&quot;, although it would not even seem applicable for sites such as cnn.com that publish continuously.</p>
<p>One of the primary objectives of NexentaEdge is to be a shared repository for versioned documents that can be accessed and updated by thousands of tenant approved users. Any tenant-approved user should be able to post a new object version, subject to tenant-specified ACLs, at any time without interference from other users. Any tenant-approved user should be able to fetch any version of any tenant object at any time without interference from other users beyond contention for bandwidth. Information about new object versions is propogated asynchronously, but rapidly, and with known and measured propogation delay.</p>
<p>A storage service, as opposed to a publishing service, needs to treat stored payload as opaque blobs. The storage service is not allowed to find references within the payload because it should embrace client driven end-to-end encryption. The storage service should presume that all payload is encrypted and never try to analyze it.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup></p>
<p>So information that supports finding stored object, by name or by other search criteria, must be stored as metadata separate from the payload. Metadata also serves the closely interlocked issue of how and even whether to retain content.</p>
<h2><a class="anchor" aria-hidden="true" name="immutable-version-metadata"></a><a href="#immutable-version-metadata" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Immutable Version Metadata</h2>
<p>By definition, most metadata about a specific version of an object must be immutable. Certain metadata can be independent of the version contents, such as metadata controlling retention of the object version. We can meaningfully talk about changing erasure encoding algorithm used to store a specific document version, but if we are changing the Author of the document we are creating a new version.</p>
<p>In particular, whether or not a given version is the current version of the object is obviously subject to change without changing the version itself. One of the strong points for IPFS is that it does not change the storage for a directory object when the mutable naming reference is changed to point at a new version. This is far preferable to the practice of creating an explicitly versioned name for non-current versions, such as used by Swift object storage.</p>
<p>However, there are many features of the Metadata system required for versioned document storage that IPFS simply does not address:</p>
<ul>
<li>Single Step searches.</li>
<li>Directory/Folder searches with single edit Hierarchical Path Edits.</li>
<li>New Metadata must be propagated quickly.</li>
<li>Predictable search times building upon short RTOs.</li>
<li>Tenant control over access-to and modification-of tenant metadata.</li>
<li>Metadata driven retention of metadata and refeereced Payload.</li>
</ul>
<h2><a class="anchor" aria-hidden="true" name="single-step-searches"></a><a href="#single-step-searches" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Single Step searches</h2>
<p>IPFS describes a multi-step process to resolve a pathname:</p>
<ul>
<li>The root of the path name is looked up in the mutable naming system (IPNS). That leads to a directory object encoding references.</li>
<li>Each layer of the the &quot;/&quot; delimited name is then iterated. For &quot;/A/B/C/D&quot;, &quot;B&quot; is looked up in the &quot;/A&quot; directory. &quot;C&quot; in the resulting directory, etc.</li>
<li>Finally the reference object is retrieved.</li>
</ul>
<p>This is common for &quot;distributed&quot; storage systems which have effectively just ported the Unix inode to the cloud. Iterative descent is a great theory and very general, but it has not been a performant solution for some time. Single-node storage servers work around this by caching the top level directories. Web-servers have been caching mappings of fully qualified URLs to files for some time as well. But iterative descent results in terrible performance when you have to jump to different storage servers for each step of the iteration. Once you have distributed storage it is very unlikely that the servers handling &quot;/A&quot; will be the same as the servers handling &quot;/A/B&quot;. The same applies for &quot;/A/B/C&quot;. Even if the entries are cached everywhere, the process requires too many network round trips. If the object name is &quot;/A/B/C/D&quot; the metadata system has to be able to look that up, within the context of the tenant, in a single-step search.</p>
<p>NexentaEdge can resolve a name using the TargetGroup search or a Namespace Manifest search. It involves many servers, but the search is conducted in parallel, not iteratively.</p>
<p>In both cases a single query<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> is sent either to the TargetGroup or to the Namespace Manifest shards. The addressed targets send their responses back to the Initiator.</p>
<p>The Initiator collects as many responses as are required to find the requested CHID to be retrieved.</p>
<h2><a class="anchor" aria-hidden="true" name="directory-folder-searches-with-single-edit-hierarchical-path-edits"></a><a href="#directory-folder-searches-with-single-edit-hierarchical-path-edits" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Directory/Folder searches With Single-edit Hierarchical Path Edits</h2>
<p>Like most cloud-inode solutions, IPFS supports querying directories by iterating from the root Directory until the desired layer and simply reading the directory.</p>
<p>NexentaEdge sends a query to the Namespace Manifest shards requesting all records relevant to resolving a given path. This includes &quot;rename&quot; records which allow single edit updates to hierarchical path names.</p>
<p>Recursive descent allows renaming the path to all objects by simply renaming one directory in the path. &quot;/A/B/<em>&quot; becomes &quot;/A/B2/</em>&quot; simply by renaming the &quot;B&quot; entry within the &quot;/A&quot; directory to &quot;B2&quot;. That is a lot more difficult with distributed directories in a storage cloud. If you support finding an object with its full path name then you are ultimately hashing based upon the fully qualified path name (&quot;/A/B/C/D&quot;). When you change &quot;B&quot; to &quot;B2&quot; you change the hash for all objects that are conceptually contained within &quot;/A/B&quot;. Executing that synchronously, before completing the request, would be impossible. There could be billions of objects contained within a single directory.</p>
<p>NexentaEdge solves this by creating &quot;rename&quot; entries that record when &quot;B&quot; was renamed to &quot;B2&quot;. In the worst case this may force the Initiator to issue a second search using the original folder name to guarantee that it had found all objects in &quot;/A/B2&quot;. But the path edit from &quot;/A/B&quot; to &quot;/A/B2&quot; only requires creating a single entry in the Namespace Manifest.</p>
<h2><a class="anchor" aria-hidden="true" name="new-metadata-is-propagated"></a><a href="#new-metadata-is-propagated" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>New Metadata Is propagated</h2>
<p>NexentaEdge has a two-track method for searching metadata. The search for Version Manifests can be conducted within the Negotiating Group (selected by the NHID) or by searching a sharded Namespace Manifest. The Negotiating Group search is limited to searching for an exact name, and will be limited to searching for the current version once the Namespace Manifest implementation is mature enough.</p>
<p>The Negotiating Group metadata is available as soon as it is put. The Namespace Manifest is updated by post-processing of transaction journals. Updates are sent to the Namespace Manifests shards. The source can be configured to be the initiators or the Targets that create new Version Manifests. These updates are batched. The granularity of batches is configurable. Further, the Namespace Manifest records the latest batch info from each source. This means that a query resolver knows the time as of which it knows all Version Manifests, and which Version Manifests <em>might</em> exist but not yet have been propagated.</p>
<p>IPFS, and other distributed inode solutions, either have to confirm update through the root inode (which would greatly slow down transaction speeds) or live with asynchronous upward posting of the inode tree (with no way to track when this is done). On a functioning network both solutions will propagate this data very quickly, but NexentaEdge can let the querier know when propagation has been delayed.</p>
<h2><a class="anchor" aria-hidden="true" name="predictable-search-times-building-upon-short-rtos"></a><a href="#predictable-search-times-building-upon-short-rtos" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Predictable search times building upon short RTOs</h2>
<p>NexentaEdge maintains metadata and Namespace Manifests so that the RTO to reach all required replicas/shards has a short maximum RTO. The time to resolve any query is directly determined by this RTO.</p>
<p>Other systems, including IPFS, do not guarantee that a name can be resolved within the current site. Therefore the query may be dependent on long-haul RTOs. This takes longer, and it takes longer before retry operations can begin after a failure. Combined this greatly increases the time that must be allowed to complete any query.</p>
<h2><a class="anchor" aria-hidden="true" name="tenant-control-over-access-to-and-modification-of-tenant-metadata"></a><a href="#tenant-control-over-access-to-and-modification-of-tenant-metadata" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Tenant control over access to and modification of tenant Metadata</h2>
<p>NexentaEdge enforces a two-layer access control strategy. The first layer imposes strict tenant isolation. All metadata belong to a specific tenant, and is accessible only by users approved by that tenants authentication server. The second tier is inforcement of ACL rules, where the specific rules are part of tenant supplied metadata and permissions/roles granted to the tenant approved users.</p>
<p>IPFS creates a global, visible namespace. If security is desired it must be provided by user-controlled encryption.</p>
<h2><a class="anchor" aria-hidden="true" name="metadata-driven-retention-of-metadata-and-referenced-payload"></a><a href="#metadata-driven-retention-of-metadata-and-referenced-payload" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Metadata driven retention of metadata and referenced Payload</h2>
<p>IPFS control of data retention is a bolt-on. Pinning of IPFS files is done on a per target basis. Cluster-driven retention requires execution of a RAFT-derived consensus algorithm. Requiring cluster-wide consensus for a routine operation seems to be contrary to the goal of being a scale-out storage solution.</p>
<p>NexentaEdge Chunks are retained if they are referennced. There is a MapReduce algorithm to distribute back-reference requirements. Once this information has been distributed each storage target is free to delete older chunks that have not been retained.</p>
<p>Version Manifests are retained when they are referenced in Snapshots or they are current.</p>
<p>Tenants are allowed to expung their own Version Manifests. This enables them to expunge content from their account in order to comply with legal requirements to remove content. Tenants will be able to subscribe to receive notices if expunged chunks are re-added.</p>
<h2><a class="anchor" aria-hidden="true" name="metadata-for-enterprise-storage"></a><a href="#metadata-for-enterprise-storage" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Metadata for Enterprise storage</h2>
<p>NexentaEdge's metadata is not just immutable, self-validating and location idenpendent. It supports rapid metadata searches that are designed to meet the needs of a document/object storage system holding tenant-private objects. IPFS is inherently limited to publishing the permanent web, and will never be suitable as a versioned project active archive.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1"  class="footnote-item"><p>It can try to compress the data to save storage resources, but obviously that will not work if the payload was in fact already encrypted. <a href="#fnref1" class="footnote-backref">↩</a></p>
</li>
<li id="fn2"  class="footnote-item"><p>As will be noted, having renamed directories in the queried path can require an additional query round. However, that <a href="#fnref2" class="footnote-backref">↩</a></p>
</li>
</ol>
</section>
</span></div></article></div><div class="post"><header class="postHeader"><h1><a href="/blog/2018/03/26/NamespaceManifests.html">Namespace Manifests</a></h1><p class="post-meta">March 26, 2018</p><div class="authorBlock"><p class="post-authorName"><a target="_blank">Caitlin Bestler</a></p></div></header><article class="post-content"><div><span><p>With efficient group messaging a group of storage targets can efficiently manage the collective responsibility for storing Chunks within the group while allowing metadata references to the stored chunks to omit the specific storage targets selected.</p>
<p>That can be extended to find old versions of the stored objects by having each Target track the list of versions stored for each object. But that increases the number of persistent write operations required for each new object version by one.</p>
<p>As covered in the prior blogs, each Version Manifest is immutable. That means that information about a Version Manifest is also immutable. If each Version Manifest is uniquely identified, then the records describing each Version Manifest are also uniquely identified. What NexentaEdge takes advantage of is that if you have a vast distributed collection of immutable unique records can be coalesced into fewer locations where they can be efficiently searched.</p>
<p>We call this master manifest that collects information about all Version Manifests a Namespace Manifest. Each Namespace Manifest deals with one slice of the cluster's namespace and may be sharded over multiple Target machines.</p>
<p>The sharded Namespace Manifest can be organized in a variety of ways to efficiently process more enhanced queries, such as all objects contained within a given scope name, or all object versions with names ending in &quot;.mp3&quot; created in 2015 by a specific user.</p>
<p>The only question with this asynchronous collection of information describing Version Manifests is not the data associated with any Version Manifest (it is immutable) but knowing the range of Version Manifests which <strong>might</strong> exist but could be as of yet unknown to the collected record store.</p>
<p>That can be addressed by including data from each Initiator about what cutoff date they have for new Version manifests. When Initiator X forwards data about Version Manifests it has collected in a batch it notes that it is no longer creating new Version Manifests with a timestamp prior to X.</p>
<p>The collective master manifest therefore knows that it knows all versions manifests dated earlier than these cutoff timestamps.</p>
<h2><a class="anchor" aria-hidden="true" name="snapshots"></a><a href="#snapshots" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Snapshots</h2>
<p>The Namespace Manifest can answer a query as to what the current Version Manifest was for any set of objects at one point-in-time.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> If there are potentially unknown Version Manifests at that time that it might not know of yet then this resulting subset is not yet complete. The results of such a query can be saved as a version of a Snapshot object.</p>
<p>When it is complete it is a true point-in-time snapshot of a distributed cluster that never stalls any Initiator from creating new object versions because of network issues or the actions of any other initiator.</p>
<p>In photographic terms this is a true point-in-time snapshot, you just have to develop the film before you can make a print. That developing time is the lag time required to collect the records.</p>
<p>Most &quot;snapshots&quot; of distributed storage are anything but &quot;snapshots&quot;. They may require a cluster-wide &quot;freeze&quot; to take the snapshot.</p>
<p>Chandry and Lamport in their 1985  <sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> compare the problem of taking a snapshot of a distributed system to that of taking a photograph of a sky full migrating birds. The image is to immense to be captured by a single photograph, and the photographers cannot demand the birds &quot;freeze&quot; to enable photo gathering.</p>
<p>Others merely supporting creating clones of a specific object version and call the clone a &quot;snapshot&quot;.</p>
<p>NexentaEdge provides a true distributed snapshot. Chandry and Lamport algorithm requires end-to-end communication. Ours does not require end-to-end communication to take the snapshot, merely to publish it.</p>
<p>Because all of the information about a Version Manifest is unique and immutable a Snapshot can cache any portion of the information form the Version Manifest in the snapshot itself. While this makes the snapshot object larger, it can speed up access to the snapshot objects. This can allpw distributed compute jobs to publish results as a snapshot, allowing single-step access to the referenced chunks by clients who effectively &quot;mount&quot; the snapshot.</p>
<h2><a class="anchor" aria-hidden="true" name="not-block-chain"></a><a href="#not-block-chain" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Not Block-Chain</h2>
<p>The fact that our metadata is immutable and additive might cause some to think of it as being similar to Blockchain algorithms. There is an important difference: we alway allow any Initiator to create a new version of any object (constrained only by the limitation of 1 new version per Initiator per Object per tick). This means that the one-tick rule is the <em>only</em> bottlneck to the creatiaon of new object versions. Block-0chain requires each new ledger entry to be authenticated through the deliberately expensive &quot;mining&quot; process that creates a major bottleneck on the recording of new ledger entries.</p>
<h2><a class="anchor" aria-hidden="true" name="all-derived-from-unique-immutable-metadata"></a><a href="#all-derived-from-unique-immutable-metadata" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>All Derived from Unique Immutable metadata</h2>
<p>The benefits outlined here are all enabled by the definition of NexentaEdge metadata. The methods of collecting, indexing and publishes these derivatives will vary as NexentaEdge evolves as a product. But all of these solutions are enabled by the fact that the information about a Version Manifest can never become obsolete.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1"  class="footnote-item"><p>This requires following certain rules on how you timestamp things, such as never allowing a clock to run backwards and starting with fairly well synchronized clocks. <a href="#fnref1" class="footnote-backref">↩</a></p>
</li>
<li id="fn2"  class="footnote-item"><p>Leslie Lamport, K. Mani Chandy: Distributed Snapshots: Determining GlobalStates of a Distributed System.
In: ACM Transactions on Computer Systems 3. Nr. 1, Februar 1985 <a href="#fnref2" class="footnote-backref">↩</a></p>
</li>
</ol>
</section>
</span></div></article></div><div class="post"><header class="postHeader"><h1><a href="/blog/2018/03/22/LocationIndependentReferences.html">Location Independent References</a></h1><p class="post-meta">March 22, 2018</p><div class="authorBlock"><p class="post-authorName"><a target="_blank">Caitlin Bestler</a></p></div></header><article class="post-content"><div><span><p>In the prior blog on NexentaEdge we mentioned that
Chunks were unique and immutable and that Chunk References
merely identify how a Chunk is used to rebuild an object,
but do not specify the locations where the chunk is stored.</p>
<p>This time we will expand on how the Location Independent
References are done.</p>
<p>The Version Manifest specifies a specific version of an object. It specifies the metadata for the version, including a few mandatory fields, and a series of Chunk References which reference the payload chunks.</p>
<p>A typical Chunk Reference contains:</p>
<ul>
<li>The CHID of the referenced chunk.</li>
<li>The Logical Offset of the Chunk in the object version.</li>
<li>The Logical Length of the decompressed payload.</li>
</ul>
<p>What it does not specified is any locations where the replicas are held. This means that the content can be migrated either for maintenance or load-balancing purposes without updating the Version Manifest.</p>
<p>Actually lots of systems have location-free Chunks
References. What is different about NexentaEdge is
that the location-free Chunk References can specify
a dynamic set of locations that can change without
the add or drop of any storage target.</p>
<p>This is done by hashing the relevant cryptographic
hash (content or name) to a Negotiating Group rather
than to a set of target machines. Storing and
retrieving chunks is negotiated within the group.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup></p>
<p>We'll explain the four most critical transactions that implement this strategy:</p>
<ul>
<li>Getting a Payload Chunk</li>
<li>Putting a Payload Chunk</li>
<li>Getting a Version Manifest</li>
<li>Putting a Version Manifest</li>
</ul>
<h2><a class="anchor" aria-hidden="true" name="get-chunk-with-chid"></a><a href="#get-chunk-with-chid" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Get Chunk with CHID</h2>
<pre><code class="hljs css mermaid">sequenceDiagram
Initiator-&gt;&gt;TargetGroup: <span class="hljs-builtin-name">Get</span> Chunk with <span class="hljs-attribute">CHID</span>=X
TargetGroup-&gt;&gt;Initiator: Have Chunk Can Deliver at T | <span class="hljs-keyword">Not</span> here<span class="hljs-built_in">
Note </span>left of TargetGroup: Response is <span class="hljs-keyword">from</span> each Target <span class="hljs-keyword">in</span> TargetGroup<span class="hljs-built_in">
Note </span>over Initiator: Select best offer
Initiator-&gt;&gt;TargetGroup: Select Target <span class="hljs-keyword">to</span> Supply Chunk<span class="hljs-built_in">
Note </span>over TargetGroup: Wait till specified time
TargetGroup-&gt;&gt;Initiator: Requested Chunk<span class="hljs-built_in">
Note </span>left of TargetGroup: <span class="hljs-keyword">From</span> the selected target<span class="hljs-built_in">
Note </span>over Initiator: Initiator validates received chunk, retries on error.
</code></pre>
<p>Payload chunks are found by sending a find
request identifying the CHID (Content Hash IDentifier)
of the desired chunk to every member of the TargetGroup. This target  group is selected by hashing the CHID.</p>
<p>Each receiving Target responds to the Initiator with
either an indication that it has Chunk X and could
deliver it at time Y, or that it does not have Chunk X.</p>
<p>Once sufficient replies have been received to make
a selection the Initiator sends a message to the TargetGroup specifying the selection it has made.
This is sent to the same group so that nodes not selected can
cancel tentative resource reservations.</p>
<p>Lastly the selected storage target delivers the requested
chunk at the specified time. Because this was negotiated,
a network with a non-blocking core can transmit the chunks
at the full rate provisioned for payload transfers.</p>
<h2><a class="anchor" aria-hidden="true" name="put-chunk-with-chid"></a><a href="#put-chunk-with-chid" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Put Chunk With CHID</h2>
<pre><code class="hljs css mermaid">sequenceDiagram
Initiator-&gt;&gt;TargetGroup: Put Chunk with <span class="hljs-attribute">CHID</span>=X
TargetGroup-&gt;&gt;Initiator: Could Accept at Time I-J | Already Stored<span class="hljs-built_in">
Note </span>left of TargetGroup: Response is <span class="hljs-keyword">from</span> each Target <span class="hljs-keyword">in</span> TargetGroup<span class="hljs-built_in">
Note </span>over Initiator: Select best <span class="hljs-builtin-name">set</span> of Targets
Initiator-&gt;&gt;TargetGroup: Select Targets <span class="hljs-keyword">to</span> Receive Chunk at Time T<span class="hljs-built_in">
Note </span>over Initiator: Wait till specified time
Initiator-&gt;&gt;TargetGroup: Chunk
TargetGroup-&gt;&gt;Initiator: Receipt Ack<span class="hljs-built_in">
Note </span>Left of TargetGroup: Optional Receipt Ack <span class="hljs-keyword">from</span> each receiving Target
TargetGroup-&gt;&gt;Initiator: Chunk Saved Ack<span class="hljs-built_in">
Note </span>Left of TargetGroup: Chunk Saved Ack <span class="hljs-keyword">from</span> each receiving Target<span class="hljs-built_in">
Note </span>over Initiator: Initiator Retries unless sufficient replicas were confirmed
</code></pre>
<p>Of course before we can get Chunk X from somewhere
within a TargetGroup we have to put it to that
group.</p>
<p>Each member of the group identifies when it could
accept the transfer. The Initiator picks the best
set of targets with an overlapping delivery window
to receive the required number of replicas.</p>
<p>The number of replicas can be reduced when some
replicas already exist. This message can also
complete the transaction if there are already
sufficient replicas.</p>
<p>There is also a nearly identical Replicate Chunk
transaction to test if there are sufficient replicas
of an already existing Chunk and to put this missing
replicas if there is not.</p>
<h2><a class="anchor" aria-hidden="true" name="get-version-manifest-with-nhid"></a><a href="#get-version-manifest-with-nhid" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Get Version Manifest With NHID</h2>
<pre><code class="hljs css mermaid">sequenceDiagram
Initiator-&gt;&gt;TargetGroup: <span class="hljs-builtin-name">Get</span> Version Manifest with <span class="hljs-attribute">NHID</span>=X
TargetGroup-&gt;&gt;Initiator: Have Version Manifest with UVID X Can Deliver at T | <span class="hljs-keyword">Not</span> here<span class="hljs-built_in">
Note </span>left of TargetGroup: Response is <span class="hljs-keyword">from</span> each Target <span class="hljs-keyword">in</span> TargetGroup<span class="hljs-built_in">
Note </span>over Initiator: Select best offer
Initiator-&gt;&gt;TargetGroup: Select Target <span class="hljs-keyword">to</span> Supply Version Manifest<span class="hljs-built_in">
Note </span>over TargetGroup: Wait till specified time
TargetGroup-&gt;&gt;Initiator: Requested Version Manifest<span class="hljs-built_in">
Note </span>left of TargetGroup: <span class="hljs-keyword">From</span> the selected target<span class="hljs-built_in">
Note </span>over Initiator: Initiator validates received Version Manifest, retries on error.<span class="hljs-built_in">
Note </span>over Initiator: Typically then fetch the referenced chunks.
</code></pre>
<p>Of course a storage system that only allowed you to retrieve content previously stored if you remembered a 256 or 512 arbitrary identifier wouldn't be very useful. We need to put and get named objects. Typically we want the current version of a named object.</p>
<p>Each object version is described by a Version Manifest. Version Manifests are also Chunks, but they are assigned to TargetGroups based upon their fully qualified object name (it is fully qualified because what the tenant perceives of as the &quot;Fully Qualified&quot; name is prefixed by the Tenant name).</p>
<p>Current Version Manifests are found by sending a
named find requesting identifying the NHID (Name hash
IDentier) of the Version Manifest desired. This is send to the TargetGroup hashed from the NHID. The default request
seeks the most current version stored by each target in the group.
The Group is derived from the NHID rather than the CHID.</p>
<p>Each receiving Target responds saying it could deliver
a Version Manifest with NHID X and UVID Y (the unique
version identifier, including the version's timestamp.
It is made unique by adding the original Initiator's
IP address as a tie-breaker).
Each is the most current version known to that Target.</p>
<p>Once sufficient replies have been collected, the
Initiator selects the Version Manifest it wants,
and sends a message to the TargetGroup speciyfing which
Target should supply the Version Manifest and at what time.
Again, this allows the non-selected targets to release tentative resource claims.</p>
<p>Lastly the selected storage target delivers the selected
Version Manifest to the Initiator at the negotiated
time at the configured full rate.</p>
<h2><a class="anchor" aria-hidden="true" name="put-version-manifest"></a><a href="#put-version-manifest" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Put Version Manifest</h2>
<pre><code class="hljs css mermaid">sequenceDiagram
Initiator-&gt;&gt;TargetGroup: Put Version Manifest with <span class="hljs-attribute">NHID</span>=X
TargetGroup-&gt;&gt;Initiator: Could Accept Delivery at Times I - J<span class="hljs-built_in">
Note </span>left of TargetGroup: Response is <span class="hljs-keyword">from</span> each Target <span class="hljs-keyword">in</span> TargetGroup<span class="hljs-built_in">
Note </span>over Initiator: Select best <span class="hljs-builtin-name">set</span> of Targets
Initiator-&gt;&gt;TargetGroup: Select Target <span class="hljs-builtin-name">Set</span> <span class="hljs-keyword">to</span> store Version Manifest at time T<span class="hljs-built_in">
Note </span>over Initiator: Wait till specified time
Initiator-&gt;&gt;TargetGroup: Version Manifest<span class="hljs-built_in">
Note </span>left of TargetGroup: <span class="hljs-keyword">To</span> each Target previously selected
TargetGroup-&gt;&gt;Initiator: Receipt Ack<span class="hljs-built_in">
Note </span>Left of TargetGroup: Optional Receipt Ack <span class="hljs-keyword">from</span> each receiving Target
TargetGroup-&gt;&gt;Initiator: Chunk Saved Ack<span class="hljs-built_in">
Note </span>Left of TargetGroup: Chunk Saved Ack <span class="hljs-keyword">from</span> each receiving Target<span class="hljs-built_in">
Note </span>over Initiator: Initiator Retries unless sufficient replicas were confirme
</code></pre>
<p>Putting a new Version Manifest is nearly identical
to putting a Payload Chunk, except that the Put
request is sent to the NHID-derived group
(rather than CHID-derived) and that there will
not be a pre-existing Version Manifest with the
same UVID (Unique Version IDentifier).</p>
<h2><a class="anchor" aria-hidden="true" name="dealing-with-old-versions-and-more"></a><a href="#dealing-with-old-versions-and-more" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Dealing With Old Versions and More</h2>
<p>The early releases of NexentaEdge implemented Version searches by having each Target maintain a list of Version Manifests they stored for each Object they stored.</p>
<p>We have a new approach that uses a two track system:</p>
<ul>
<li>The Targets only track the current version. This is the most common version requested, and we save one persistent storage write for each new object version by only tracking the current version.</li>
<li>A &quot;Namespace Manifest&quot; which is a distributed object that uses MapReduce techn\niques to collect and query a distributed key-value store of all Version Manifests logged by any target in the cluster.</li>
</ul>
<p>Namespace Manifests enable doing queries on any directory, or even any wildcard mask. Other object stores use some equivalent of Swift's ContainerDB to enumerate all versions within a single container. The Namespace Manifest allows queries for <em>any</em> directory, not just the root directories. It also allows the Namespace Manifest to be updated asynchronously, but reliably.</p>
<p>We'll cover the Namespace Manifest next time, and then how the Namespace Manifest enables true point-in-time snapshots even in a cluster with no cluster-wide synchronization.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1"  class="footnote-item"><p>This is done with multicast groups confined to the backend network by default, or by iterative unicasting otherwise. <a href="#fnref1" class="footnote-backref">↩</a></p>
</li>
</ol>
</section>
</span></div></article></div><div class="post"><header class="postHeader"><h1><a href="/blog/2018/03/20/ConsensusNotNeeded.html">Consensus, Who Needs It?</a></h1><p class="post-meta">March 20, 2018</p><div class="authorBlock"><p class="post-authorName"><a target="_blank">Caitlin Bestler</a></p></div></header><article class="post-content"><div><span><p>The conventional tradeoff for distributed storage clusters is between transactional consistency and eventual consistency. Eventual consistency is usually viewed as the cheaper solution, both in terms of desirability and system cost. The critical cost of transactional consistency is the need to reach a consensus on ordering updates.</p>
<p>Eventual consistency is usually portrayed as simply tolerance for inconsistency on the presumption that momentary contradictions are acceptable as long as they go away eventually.</p>
<p>NexentaEdge takes a different approach. All stored chunks, whether metadata or payload, are unique, immutable and self-validated. References to these chunks do not include the locations where they are stored, but still enable those chunks to be efficiently retrieved.</p>
<p>This strategy allows NexentaEdge to provide guarantees beyond making thing consistent &quot;eventually&quot;:</p>
<ul>
<li>Any client will never retrieve a version older than the most recent version that the client has put itself.
The changes in any version will never be automatically erased. * A version will only be expunged according to policy and after a version that is a successor to it is published.</li>
<li>No network partition will prevent a client from putting a new object version. Indeed no client will ever prevent another client from putting a new object version.</li>
</ul>
<p>Never blocking a new object version because of the actions of another client is a feature, not a bug or limitation.  Transactional systems can only guarantee non-overlapping edits after reaching a consensus. The consensus may be on which updater has the exclusive right to update an object now (distributed locking) or on which of multiple conflicting updates can be committed (MVCC or multi-version consensus control). Effectively the cluster must be serialized either before the update is initiated or before it can be completed. Distributed locking is more efficient when conflicting attempted edits are common, MVCC for the far more common situation where conflicts are rare.</p>
<p>So eventual consistency is actually what end consumers want for versioned document archives. Not accepting, or even delaying, the ability to record a new version is not good. What they would prefer is to reliably know when other versions are created and to minimize the time when a new update is not visible to someone wanting to further edit the same document.</p>
<p>What NexentaEdge offers is eventual consistency with the benefits of immutability and knowledge of what range of possible object versions could exist but which have not yet been propagated.</p>
<p>What lies behind this capability is simple, NexentaEdge has defined its metadata so that no consensus algorithm is needed. Other storage solutions may have clever consensus algorithms, but you cannot be more clever than no consensus algorithm at all.</p>
<h2><a class="anchor" aria-hidden="true" name="consensus-is-expensive"></a><a href="#consensus-is-expensive" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Consensus is Expensive</h2>
<p>The fundamental issue is that it is impossible to update the same information at multiple locations at exactly the same time. This has been expressed many ways, including CAP Theorem.</p>
<p>Distributed Storage systems that offer transactional consistency by requiring a cluster-wide consensus before the put transaction creating a new object version can complete. This may be based upon <em>a priori</em> locks or optimistic locking which detects conflicting edits and immediately applies the conflicting edits before reapplying the attempted edit.</p>
<p>Either strategy requires end-to-end communications covering at least a relevant quorum of node members. Of course, a quorum based consensus is dependent on agreement about how many votes are needed, which is why consensus algorithms always get complex. If a quorum consensus on either the lock or the specific edit cannot be achieved then the requested operation cannot proceed or complete. Disallowing puts of new versions is the <em>last</em> thing that a storage cluster supporting versioned documents should do.</p>
<h2><a class="anchor" aria-hidden="true" name="unique-chunks-do-not-require-consensus"></a><a href="#unique-chunks-do-not-require-consensus" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Unique Chunks Do Not Require Consensus</h2>
<p>NexentaEdge defines object versions in metadata chunks called Version Manifests. These chunks include the fully qualified object name and a unique version identifier.</p>
<p>Chunks are located using a cryptographic hash identifier of the chunk. For Version Manifests this is the Name Hash Identifier (NHID). All Version Manifests for a given object are stored within storage servers addressed by a single multicast group derived from the NHID. Payload Chunks, by contrast, are located based on the Content Hash Identifier (CHID). Depending on options selected the cryptographic hashes may be 256 or 512 bits.</p>
<p>Further, because the Version Manifests include their unique identifier, their Content Hash Identifiers (CHIDs) are also unique.</p>
<h2><a class="anchor" aria-hidden="true" name="nexentaedge-always-accepts-new-versions"></a><a href="#nexentaedge-always-accepts-new-versions" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>NexentaEdge Always Accepts New Versions</h2>
<p>All NexentaEdge chunks are unique. They either have unique payload identified by a 256 or 512 bit cryptographic hash, or they have a Version Manifest that includes a unique identifier of the the version.</p>
<p>Two nodes can both put the same payload chunks without harm. Because the unique version identifier includes the IP address of the originating node there is only a single source for any new version. This does impose the onerous constraint that no single source can put two versions of the same object within a single system tick, currently 1/10,000th of a second. The round trip times to negotiate and confirm putting a new chunk will take longer than that.</p>
<p>Because the same Version Manifest has a unique identifier the source creating it does not need to consult with any other node before creating it. The only entity that is qualified to have an opinion on whether it created a new version of a given object at a given timestamp is itself. Instant consensus.</p>
<h1><a class="anchor" aria-hidden="true" name="namespace-manifest-and-snapshots"></a><a href="#namespace-manifest-and-snapshots" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Namespace Manifest and Snapshots</h1>
<p>Because Version Manifests are unique they can always be created. NexentaEdge collects and processes the transaction log entries noting each new Version Manifest to create a permanent registry of all Version Manifest that we call a Namespace Manifest. The Namespace Manifest can support complex metadata queries and makes it possible to take true point-in-time snapshots of a distributed storage cluster without requiring any consensus deriving blockage.</p>
<p>We'll follow up on the Namespace Manifest and Snapshots in our next blog.</p>
</span></div></article></div><div class="post"><header class="postHeader"><h1><a href="/blog/2016/03/11/blog-post.html">One SDS feature that made ZFS famous</a></h1><p class="post-meta">March 11, 2016</p><div class="authorBlock"><p class="post-authorName"><a href="http://twitter.com/dmitryy" target="_blank">Dmitry Yusupov</a></p></div></header><article class="post-content"><div><span><p>It was back in 2005 when Sun Microsystems unveiled OpenSolaris and an with it young yet brave new file system called ZFS. Clearly Sun did lots of marketing to get ZFS widely recognized and known. Mostly it was grass root effort via engineering blogs and forums, people who were very passionate about what they've architected and engineered.</p>
<p>But what was it that made ZFS so prominent among other existing filesystems at the time? What is it was wrong with the world that made it widely accepted?</p>
<p>Some believe that ZFS’s differentiation was a direct cause from its performance. However, ZFS wasn't the fastest. In fact, It was on average acceptable in majority of workloads and EXT4 did outperform ZFS actually. ZFS was greedy for available DRAM and CPU resources. And when over time these components became affordable, following Moore's low curve, ZFS outperformed EXT4 on write intensive workloads. The rectified performance of the ZFS system, however, was not the leading reason for its fame.</p>
<p>ZFS stands for &quot;Zettabyte File System&quot;. But i didn't see that its great addressability what was critical at a time. In fact, typical array size (ZFS pool cannot be accessed via two nodes simultaneously) was in range of 100-200TBs. You do not need ZB+ addressability if all you planning to address is just going to be within 1PB at most. No, it wasn't its great scalability either..</p>
<p>Features? Usability? Well, that did help to attract hundreds of thousands of enthusiasts to play with it. It certainly gave it necessary momentum. But this couldn't compare to what Linux adoption did with millions of people using EXT4 every day till these days actually. And by the way, breaking layers idea was &quot;too revolutionary&quot; at a time.. while gaining better usability (LVM and FS layer integrated) it did create unnecessary controversy, along side with CDDL vs. GPL discussions. This didn't help much.</p>
<p>Looking back, I'd say ZFS the most important feature that really made it so appreciated in enterprise circles is its built-in end-to-end integrity as a result of Copy-On-Write technique it used so masterfully. As a matter of fact this technique was the core dispute between NetApp and Sun (later Oracle) until it was court settled due to patent claims expiration dates.</p>
<p>To understand this, you need to be an admin or a devop fellow who's job is on the hook if their enterprise would suddenly loose data or it would corrupt archives silently. When ZFS released to public, in open, free as in speech, or with support contracts from companies like Nexenta, it was big deal to those guys! No longer they need to buy expensive EMC subscriptions and NetApp contracts. ZFS let people keep critical data safe, with guaranteed integrity and self-healing capabilities built-in. Snapshots and Clones were obvious by-products of Copy-On-Write rather than separate inspirations. ZFS just harvested its benefits smartly.. by enabling ARC cache and clever transactions algorithms, it did deliver acceptable performance even for workloads that typically hard to achieve with Copy-On-Write, i.e. random writes. As i mentioned earlier Moor's low worked in their advantage.</p>
<p>It was year of 2011 when group of talented engineers and architects at Nexenta realized that world needs to extend Copy-On-Write technique to stay relevant in the Cloud era. This is when Cloud-Copy-On-Write technique was born and with it new product, delivering it - NexentaEdge.</p>
<p>Learn more about NexentaEdge at <a href="http://nexentaedge.io">http://nexentaedge.io</a>.</p>
</span></div></article></div><div class="docs-prevnext"></div></div></div></div></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="/" class="nav-home"><img src="/img/logo-nexenta-edge.png" alt="NexentaEdge" width="66" height="58"/></a><div><h5>Docs</h5><a href="/docs/en/introduction.html">Getting Started</a><a href="https://nexenta.com/products/nexentaedge">Enterprise Documentation</a><a href="https://edgex.docs.apiary.io/">Edge-X S3 API Reference</a></div><div><h5>Community</h5><a href="https://twitter.com/nexenta" target="_blank"><i class="fab fa-twitter fa-sm fa-fw"></i> Twitter</a><a href="https://join.slack.com/t/nexentaedge/shared_invite/enQtMzEwMjA5MTczNDU3LTVmNjk4NjEwNTVlYThjMjg4NWI0ZWM5NTBjNTE5YzgwZTFjYjhjMWFhMWY4NjYxYWI0YWJmOTFkNTY5MmI1YWI" target="_blank"><i class="fab fa-slack fa-sm fa-fw"></i> Slack</a><a href="https://groups.google.com/forum/#!forum/nexentaedge-users" target="_blank"><i class="fab fa-google fa-sm fa-fw"></i> Google Group</a></div><div><h5>More</h5><a href="/blog"><i class="fas fa-book fa-sm fa-fw"></i> Blog</a><a href="https://github.com/Nexenta/nedge-dev"><i class="fab fa-github fa-sm fa-fw"></i> GitHub</a><a class="github-button" href="https://github.com/Nexenta/nedge-dev" data-icon="octicon-star" data-count-href="/facebook/docusaurus/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star</a></div></section><section class="copyright">Copyright © 2018 Nexenta Systems, Inc.</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
              var search = docsearch({
                apiKey: '839b05a95d1375c54722a0161e78d578',
                indexName: 'nexentaedge',
                inputSelector: '#search_input_react'
              });
            </script></body></html>